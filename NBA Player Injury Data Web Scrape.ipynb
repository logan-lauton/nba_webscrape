{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4de2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Logan Lauton\n",
    "\n",
    "##importing packages needed for scrape\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "##function designed to scrape all of the injuries reported in NBA history \n",
    "def single(tot):\n",
    "    ##number that determines where the start is, or in other words what page of the table the url is \n",
    "    ##as it increments by 25, page 1 => start=0 page 2 => start=25 page 3 => start=50 etc\n",
    "    num = 0\n",
    "    ##supress warnining for using df.append rather than pd.concat\n",
    "    warnings.filterwarnings('ignore', message='The frame.append method is deprecated')\n",
    "    while tot != 0:\n",
    "        if num == 0:\n",
    "            url = f'https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate=&EndDate=&ILChkBx=yes&Submit=Search&start={num}'\n",
    "            response = requests.get(url)\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            table = soup.find('table', attrs={'class': 'datatable center'})\n",
    "            ##here I create the df that will be populated throughout the scrape\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            num = num + 25           ##increment to 25 to move onto page 2\n",
    "            tot = tot - 1            ##reduce tot to see how many pages are left to visit\n",
    "            time.sleep(2.38)         ##rate limit to get the total corpus(n=1507) in 1 hour (≈ 25.21 requests/minute)\n",
    "        else:\n",
    "            url = f'https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate=&EndDate=&ILChkBx=yes&Submit=Search&start={num}'\n",
    "            response = requests.get(url)\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            table = soup.find('table', attrs={'class': 'datatable center'})\n",
    "            ##here I append to the df that was created \n",
    "            df = df.append(pd.read_html(str(table))[0])\n",
    "            num = num + 25           ##increment the start to go to the next page\n",
    "            tot = tot - 1            ##reduce the total to determine number of remaining pages\n",
    "            time.sleep(2.38)         ##rate limit to get the total corpus(n=1507) in 1 hour (≈ 25.21 requests/minute)\n",
    "\n",
    "    df.columns = ['Date','Team','Acquired','Relinquished','Notes']\n",
    "    df = df[df['Date']!='Date']\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.Acquired = df.Acquired.str.replace('•','') # remove • in front of player's name\n",
    "    df.Relinquished = df.Relinquished.str.replace('•','') # remove • in front of player's name\n",
    "    df['Date'] = pd.to_datetime(df['Date'].str.strip(), format='%Y-%m-%d')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48fff26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = single(1507)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e89a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##exporting df as csv\n",
    "df.to_csv('NBA Player Injury Stats(1951 - 2023).csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
